Tech-stack required to install:
streamlit==1.38.0 
langchain 
langchain-community 
transformers 
torch 
bitsandbytes 
faiss-cpu 
pdfplumber 
python-docx 
sentence-transformers
huggingface_hub 
streamlit_chat 
pyngrok==7.2.0 

Checks for CUDA-enabled GPU availability:
import torch
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    torch.cuda.empty_cache()
else:
    raise RuntimeError("Switch to T4 GPU runtime: Runtime > Change runtime type > T4 GPU")

To retrieve a Hugging Face API token and configures the cache directory for Hugging Face libraries:
import os
from google.colab import userdata
os.environ["HUGGINGFACEHUB_API_TOKEN"] = userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HF_HOME"] = "/content/cache"

Directory making in Google Colab:
!mkdir -p .streamlit
!mkdir -p utils
!mkdir -p /content/cache

Ngrok hosting:
from pyngrok import ngrok
from google.colab import userdata
ngrok.set_auth_token(userdata.get("NGROK_AUTH_TOKEN"))
public_url = ngrok.connect(8501)
print(f"Streamlit URL: {public_url}")
!streamlit run app.py --server.port 8501
